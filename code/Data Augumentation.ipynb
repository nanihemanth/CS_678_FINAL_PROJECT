{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77cbe6a8-88cf-4d06-bd0c-9963004636e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63d8b22-c9d6-41ea-9aa0-44da67aefe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/76/53/7f30dd413a607224b9ccc7b8b27a311095cbb7c3058c069971dc95ec7844/tokenizers-0.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/8b/50/e72b8adba500c8d09f768953196378eac2a119717cc90b2b6b14e044ad44/safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from transformers) (4.63.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/htiyyagu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/htiyyagu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed fsspec-2023.10.0 huggingface-hub-0.19.4 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69ee847-ba1f-47ba-89e6-9435c81f6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"hsv_train.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea0a929-e47c-43d7-ba79-5dcb9264369e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>english_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ha_train_00001</td>\n",
       "      <td>@user Da kudin da Arewa babu wani abin azo aga...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Da kudin da Arewa babu wani abin azo agani da ...</td>\n",
       "      <td>The Northern currency there is nothing azzarah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ha_train_00002</td>\n",
       "      <td>@user Kaga wani Adu ar Bandaüíîüò≠ wai a haka Shi ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kaga wani Adu ar Banda wai a haka Shi ne shuga...</td>\n",
       "      <td>Abain An Aduj besides why the leader of true B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ha_train_00003</td>\n",
       "      <td>@user Sai haquri fa yan madrid daman kunce cha...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sai haquri fa yan madrid daman kunce champion ...</td>\n",
       "      <td>Lo his heart, the Champion of his Champion had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ha_train_00004</td>\n",
       "      <td>@user Hmmm yanzu kai kasan girman allah daxaka...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Hmmm yanzu kai kasan girman allah daxakace muk...</td>\n",
       "      <td>Hmmm is now the bottom of God's greatest honor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ha_train_00005</td>\n",
       "      <td>@user @user Wai gwamno nin Nigeria suna afa kw...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Wai gwamno nin Nigeria suna afa kwayoyi ko</td>\n",
       "      <td>Nigeria governs Nigeria are springs of drugs or</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                              tweet  \\\n",
       "0  ha_train_00001  @user Da kudin da Arewa babu wani abin azo aga...   \n",
       "1  ha_train_00002  @user Kaga wani Adu ar Bandaüíîüò≠ wai a haka Shi ...   \n",
       "2  ha_train_00003  @user Sai haquri fa yan madrid daman kunce cha...   \n",
       "3  ha_train_00004  @user Hmmm yanzu kai kasan girman allah daxaka...   \n",
       "4  ha_train_00005  @user @user Wai gwamno nin Nigeria suna afa kw...   \n",
       "\n",
       "      label                                       cleaned_text  \\\n",
       "0  negative  Da kudin da Arewa babu wani abin azo agani da ...   \n",
       "1  negative  Kaga wani Adu ar Banda wai a haka Shi ne shuga...   \n",
       "2  negative  Sai haquri fa yan madrid daman kunce champion ...   \n",
       "3  negative  Hmmm yanzu kai kasan girman allah daxakace muk...   \n",
       "4  negative         Wai gwamno nin Nigeria suna afa kwayoyi ko   \n",
       "\n",
       "                                  english_translated  \n",
       "0  The Northern currency there is nothing azzarah...  \n",
       "1  Abain An Aduj besides why the leader of true B...  \n",
       "2  Lo his heart, the Champion of his Champion had...  \n",
       "3  Hmmm is now the bottom of God's greatest honor...  \n",
       "4    Nigeria governs Nigeria are springs of drugs or  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b58444-d4c5-46ea-b61a-77bd51efe116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "tweet                 0\n",
       "label                 0\n",
       "cleaned_text          0\n",
       "english_translated    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True,axis=0)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff7c893-55f3-4e82-a00a-8f80cf297930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>english_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ha_train_00001</td>\n",
       "      <td>@user Da kudin da Arewa babu wani abin azo aga...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Da kudin da Arewa babu wani abin azo agani da ...</td>\n",
       "      <td>The Northern currency there is nothing azzarah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ha_train_00002</td>\n",
       "      <td>@user Kaga wani Adu ar Bandaüíîüò≠ wai a haka Shi ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kaga wani Adu ar Banda wai a haka Shi ne shuga...</td>\n",
       "      <td>Abain An Aduj besides why the leader of true B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ha_train_00003</td>\n",
       "      <td>@user Sai haquri fa yan madrid daman kunce cha...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sai haquri fa yan madrid daman kunce champion ...</td>\n",
       "      <td>Lo his heart, the Champion of his Champion had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ha_train_00004</td>\n",
       "      <td>@user Hmmm yanzu kai kasan girman allah daxaka...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Hmmm yanzu kai kasan girman allah daxakace muk...</td>\n",
       "      <td>Hmmm is now the bottom of God's greatest honor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ha_train_00005</td>\n",
       "      <td>@user @user Wai gwamno nin Nigeria suna afa kw...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Wai gwamno nin Nigeria suna afa kwayoyi ko</td>\n",
       "      <td>Nigeria governs Nigeria are springs of drugs or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10188</th>\n",
       "      <td>ha_train_14168</td>\n",
       "      <td>@user Ba wasa a fuskokinsu, may you succeed al...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Ba wasa a fuskokinsu may you succeed always guys</td>\n",
       "      <td>Not playing on their faces may you have suiced...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10189</th>\n",
       "      <td>ha_train_14169</td>\n",
       "      <td>@user Allah yasa muyi kyakkyawan qarshe ü§≤ü§≤</td>\n",
       "      <td>positive</td>\n",
       "      <td>Allah yasa muyi kyakkyawan qarshe</td>\n",
       "      <td>God makes us good at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10190</th>\n",
       "      <td>ha_train_14170</td>\n",
       "      <td>@user Abu nafarko gamawa da duniya lafiya kuma...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Abu nafarko gamawa da duniya lafiya kuma inaso...</td>\n",
       "      <td>It is well encouraged with the world and that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10191</th>\n",
       "      <td>ha_train_14171</td>\n",
       "      <td>@user @user @user Allah ubangiji yaiwa rayuwar...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Allah ubangiji yaiwa rayuwarsa albarka</td>\n",
       "      <td>Allah Jehovah the Lord bring His life His life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10192</th>\n",
       "      <td>ha_train_14172</td>\n",
       "      <td>@user @user Baka film din banza director Allah...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Baka film din banza director Allah ya dafa ni ...</td>\n",
       "      <td>Bow the film Director God cooked me the nasan ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10190 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                              tweet  \\\n",
       "0      ha_train_00001  @user Da kudin da Arewa babu wani abin azo aga...   \n",
       "1      ha_train_00002  @user Kaga wani Adu ar Bandaüíîüò≠ wai a haka Shi ...   \n",
       "2      ha_train_00003  @user Sai haquri fa yan madrid daman kunce cha...   \n",
       "3      ha_train_00004  @user Hmmm yanzu kai kasan girman allah daxaka...   \n",
       "4      ha_train_00005  @user @user Wai gwamno nin Nigeria suna afa kw...   \n",
       "...               ...                                                ...   \n",
       "10188  ha_train_14168  @user Ba wasa a fuskokinsu, may you succeed al...   \n",
       "10189  ha_train_14169         @user Allah yasa muyi kyakkyawan qarshe ü§≤ü§≤   \n",
       "10190  ha_train_14170  @user Abu nafarko gamawa da duniya lafiya kuma...   \n",
       "10191  ha_train_14171  @user @user @user Allah ubangiji yaiwa rayuwar...   \n",
       "10192  ha_train_14172  @user @user Baka film din banza director Allah...   \n",
       "\n",
       "          label                                       cleaned_text  \\\n",
       "0      negative  Da kudin da Arewa babu wani abin azo agani da ...   \n",
       "1      negative  Kaga wani Adu ar Banda wai a haka Shi ne shuga...   \n",
       "2      negative  Sai haquri fa yan madrid daman kunce champion ...   \n",
       "3      negative  Hmmm yanzu kai kasan girman allah daxakace muk...   \n",
       "4      negative         Wai gwamno nin Nigeria suna afa kwayoyi ko   \n",
       "...         ...                                                ...   \n",
       "10188  positive   Ba wasa a fuskokinsu may you succeed always guys   \n",
       "10189  positive                  Allah yasa muyi kyakkyawan qarshe   \n",
       "10190  positive  Abu nafarko gamawa da duniya lafiya kuma inaso...   \n",
       "10191  positive             Allah ubangiji yaiwa rayuwarsa albarka   \n",
       "10192  positive  Baka film din banza director Allah ya dafa ni ...   \n",
       "\n",
       "                                      english_translated  \n",
       "0      The Northern currency there is nothing azzarah...  \n",
       "1      Abain An Aduj besides why the leader of true B...  \n",
       "2      Lo his heart, the Champion of his Champion had...  \n",
       "3      Hmmm is now the bottom of God's greatest honor...  \n",
       "4        Nigeria governs Nigeria are springs of drugs or  \n",
       "...                                                  ...  \n",
       "10188  Not playing on their faces may you have suiced...  \n",
       "10189                               God makes us good at  \n",
       "10190  It is well encouraged with the world and that ...  \n",
       "10191     Allah Jehovah the Lord bring His life His life  \n",
       "10192  Bow the film Director God cooked me the nasan ...  \n",
       "\n",
       "[10190 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6779c4bd-6682-426d-9d47-fa6da207f192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found device: NVIDIA A100-SXM-80GB, n_gpu: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Confirm that the GPU is detected\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = torch.cuda.get_device_name()\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0decc54c-349c-4f60-a72a-11fd7ef67eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "\n",
    "MODEL_NAME = 'tuner007/pegasus_paraphrase'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51157f4d-44de-4bb8-9034-4850da4bfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(input_text, num_return_sequences):\n",
    "    batch = tokenizer.prepare_seq2seq_batch([input_text],\n",
    "                                            truncation=True,\n",
    "                                            padding='longest',\n",
    "                                            return_tensors=\"pt\").to(device)\n",
    "    translated = model.generate(**batch,\n",
    "                                num_beams=num_return_sequences,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                                temperature=1.5).to(device)\n",
    "    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21dba983-e24f-44e9-bc78-669a84648acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_df):\n",
    "    train = train_df\n",
    "    train = train[['english_translated', 'label']]\n",
    "    train['english_translated'] = train['english_translated'].apply(get_response, num_return_sequences=20)\n",
    "    generated = train.explode('english_translated')\n",
    "    generated = generated.dropna()\n",
    "    generated = generated.drop_duplicates()\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bf1b3e0-dd77-4884-a047-6e88b082cb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/htiyyagu/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3982: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "/home/htiyyagu/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1269644/1595794901.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['english_translated'] = train['english_translated'].apply(get_response, num_return_sequences=20)\n"
     ]
    }
   ],
   "source": [
    "k=main(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f256fe0-54c6-44b5-adc4-8e58df9f6a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.to_csv(\"hausa_augumented.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18fa6066-99d8-4f06-91b9-94db7186507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (3.6.7)\n",
      "Requirement already satisfied: click in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/python-3.9.9-jh/lib/python3.9/site-packages (from nltk) (4.63.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6fcdb8-6624-404d-ba22-93732cd3eee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/htiyyagu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa655869-9fb0-4321-b24d-be08f7eb81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "def find_synonym(text):\n",
    "    sentences = [text]\n",
    "    try:\n",
    "        texts = text.split()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        texts=[str(text)]\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    texts = texts[:len(texts) // 2]\n",
    "    word_bag = [i for i in texts if i not in stop_words\n",
    "                and any(map(str.isupper, i)) is False\n",
    "                and i != 'NUMERIC']  #  Exclude the numeric token and as precaution any word that is uppercased\n",
    "    word_bag = list(set(word_bag))\n",
    "    n = 2  #  each generated text will have two words replaced with their respective synonym \n",
    "    combined_bag = [word_bag[i * n:(i + 1) * n] for i in range((len(word_bag) + n - 1) // n)]\n",
    "    to_exchange = combined_bag\n",
    "    payload = text\n",
    "    try:\n",
    "        for words in to_exchange:\n",
    "            for word in words:\n",
    "                similar_words = get_synonyms(word)\n",
    "                if similar_words is not None:\n",
    "                    similar_words = [re.sub('[^A-Za-z ]+', ' ', sent) for sent in similar_words]\n",
    "                    for similar in similar_words:\n",
    "                        payload = re.sub(word, similar, payload)\n",
    "                    sentences.append(payload)\n",
    "        sentences = list(set(sentences))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f54d98d3-f6d3-45a7-9a4b-414c0901105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            if word != l.name():\n",
    "                synonyms.append(l.name())\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99fd659f-7e4a-4e07-b215-fd950a11f435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/htiyyagu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847235a3-9c43-4824-ad76-b79edf35c935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/htiyyagu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4e5ef7-3b11-4504-bc29-45b22aedc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def main_synonym():\n",
    "    train = pd.read_csv('hausa_augumented.csv')\n",
    "    train = train[['english_translated', 'label']]\n",
    "    train['english_translated'] = train['english_translated'].apply(find_synonym)\n",
    "    new_df = train.explode('english_translated')\n",
    "    new_df = new_df.dropna()\n",
    "    new_df = new_df.drop_duplicates()\n",
    "    new_df.to_csv('hausa_synonym_replaced_augmented_dataset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cf50845-b187-44d3-9daa-265e54982f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'float' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "main_synonym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f141a5b3-84da-40d3-8f18-1385c1add385",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv(\"hausa_synonym_replaced_augmented_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40eed803-3a37-4845-91ad-e90e6b15e469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_translated</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is nil azzarah God can do for the creati...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There is nothing azzarah God can do for the cr...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is nothing azzarah God can do for the cr...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There is nil azzarah God can do for the creati...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There is nil azzarah God can do for the native...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386617</th>\n",
       "      <td>The manager cook me, the nasan would match the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386618</th>\n",
       "      <td>The director cooked me, the nasan would match ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386619</th>\n",
       "      <td>The film's manager cook me the nasan would mat...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386620</th>\n",
       "      <td>The film's director cooked me the nasan would ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386621</th>\n",
       "      <td>The film's manager cooked me the nasan would m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>386622 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       english_translated     label\n",
       "0       There is nil azzarah God can do for the creati...  negative\n",
       "1       There is nothing azzarah God can do for the cr...  negative\n",
       "2       There is nothing azzarah God can do for the cr...  negative\n",
       "3       There is nil azzarah God can do for the creati...  negative\n",
       "4       There is nil azzarah God can do for the native...  negative\n",
       "...                                                   ...       ...\n",
       "386617  The manager cook me, the nasan would match the...  positive\n",
       "386618  The director cooked me, the nasan would match ...  positive\n",
       "386619  The film's manager cook me the nasan would mat...  positive\n",
       "386620  The film's director cooked me the nasan would ...  positive\n",
       "386621  The film's manager cooked me the nasan would m...  positive\n",
       "\n",
       "[386622 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73202b5-b2d5-4e82-834f-5f89e969d708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
